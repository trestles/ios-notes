


K-Nearest Neighbors P39

Figure 2-9 P31
Figure 2-10 P33
Figure 2-11 P34
Figure 2-12 P36
Figure 2-13 P38
Figure 2-14 P40
Figure 2-15 P41
Figure 2-16 P41
Figure 2-17 P42 - training error rate


Figure 2-13 P38
"KNN applies Bayes rule and classifies the test observation Xo to class with the largest probability" P39


Ch 3 Linear Regression
Advertising Data!!!



3.1 Simple Linear Regression P61
Figure 3-1 P62
Figure 3-2 Contour and 3D plots of the RSS on Advertising data using sales as the response and TV as the predictor
Figure 3-3 P64 A simulated data set

3.1.1 Estimating the Coefficients P61

least squares criterion P61


3.2 Multiple Linear Regression

3.2.1
Estimating the Regression Coefficients P73

3.2.2
One: Is there a relationship between the Response and Predictors? P75


Two: Deciding on Important Variables
Forward Selection

Backward Selection


Mixed Selection
"the p-values for variables can become larger as new predictors are added to the model" P76

Three: Model Fit


3.4 The Marketing Plan for Advertising Plan
1. Is there a relationship between advertising sales and budget? P102
2. How strong is the relationship? P102
3. Which media contribute to sales? P103
4. How large is the effect of each medium on sales? P103
5. How accurately can we predict future sales? P103
6. Is the relationship linear? P104
7. Is there synergy amongst the advertising media? P104

Figure 3-16 P105
Figure 3-17 P107
Figure 3-18 P107
Figure 3-19 P108
Figure 3-20 P109

Chapter 4

logistic regresstion, linear discriminant analysis, and K-nearest neighbors P127


4.1 An Overview of Classification P128
3 examples:
1. patient with symptoms of one of 3 diseases
2. online banking service must determine whether or not a transaction being performed is fraudulent
3. on basis of DNA sequence analysis, a biologist must figure out which DNA mutations are deleterious and which are not

4.2 Why Not Linear Regression? P129
4.3 Logistic Regression P130

Figure 4.2 Classification using the Default data

4.3.1 The Logistic Model

4.3.2 Estimating the Regression Coefficients

4.3.4 Multiple Logistic Regression P135

4.3.5 Logistic Regression for >2 Response Classes

4.4 Linear Discriminant Analysis P138
  4.4.1 Using Bayes Theorem for Classification P138
  4.4.2 Linear Discriminant Analysis for p=1
  4.4.3 Linear Discriminant Analysis for p>1 P142
    Figure 4.8 A ROC curve for the LDA classifier on the Default data P148
  4.4.4 Quadratic Discriminant Analysis P149
4.5 A Comparison of Classification Methods P151
4.6 Lab: Logistic Regression, LDA, QDA, and KNN

5. Resampling Methods P175
  5.1 Cross Validation P176
    Figure 5.1 A schematic display of the validation set approach
    Figure 5.2 The validation set approach was used on the Auto data set in order to estimate the test error that results from predicting mpg using polynomial functions of horsepower
    5.1.2 Leave-One-Out Cross-Validation
    5.1.3 k-Fold Cross Validations
    5.1.4 Bias-Variance Trade-Off for k-Fold Cross Validation
    5.1.5 Cross-Validation on Classification Problems P184
  5.2 The Bootstrap P187
    Figure 5.11 A graphical illustration of the bootstrap approach on a small sample containing n=3 observations
  5.3 Lab: Cross-Validation and the Bootstrap P190

6. Linear Model Selection and Regularization P203

  6.1 Subset Selection P205
    Figure 6.1 a subset of 10 predictors in the Credit data set P206
    6.1.2 Stepwise Selection
    6.1.3 Choosing the Optimal Model
  6.2 Shrinkage Methods
    6.2.1 Ridge Regression
      An Application to the Credit Data P217
      Why does Ridge Regression Improve Over Least Squares? P217
    6.2.2 The Lasso
  6.3 Dimension Reduction Methods P228
    6.3.1 Principal Components Regression P230
  6.4 Considerations in High Dimensions P238
    6.4.1 High Dimension Data P238
    6.4.2 What goes wrong in high dimensions? P239
    6.4.3 Regression in High Dimensions P241
    6.4.4 Interpreting Results in High Dimensions P243
